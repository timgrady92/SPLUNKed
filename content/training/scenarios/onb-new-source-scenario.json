{
  "id": "onb-new-source-scenario",
  "type": "scenario",
  "title": "Onboarding a Custom Application Log",
  "description": "Walk through the complete process of onboarding a new application log, from identifying parsing issues to CIM normalization.",
  "category": "data-onboarding",
  "difficulty": "intermediate",
  "duration": "30 min",
  "tags": ["data-onboarding", "scenario", "props.conf", "field-extraction", "CIM", "troubleshooting"],
  "objectives": [
    "Analyze raw data to identify parsing requirements",
    "Configure proper event breaking and timestamp extraction",
    "Create field extractions for key data elements",
    "Normalize fields to CIM standards",
    "Validate the complete onboarding"
  ],
  "content": {
    "situation": {
      "title": "New Application Onboarding Request",
      "description": "<p>Your security team has deployed a new cloud access security broker (CASB) that generates authentication and file access logs. The data is being collected but analysts report the events are \"unusable\" - timestamps are wrong, events are merged together, and they can't search by username or action.</p><p>You've been assigned to fix the data onboarding for this CASB log source. The raw data looks like this:</p><pre><code>===EVENT START===\nTimestamp: 2024-03-15T14:30:45.123Z\nEvent-Type: USER_LOGIN\nUser-Principal: jsmith@company.com\nSource-IP: 10.1.2.50\nApplication: Office365\nResult: SUCCESS\nRisk-Score: 15\n===EVENT END===\n===EVENT START===\nTimestamp: 2024-03-15T14:30:46.456Z\nEvent-Type: FILE_DOWNLOAD\nUser-Principal: jsmith@company.com\nSource-IP: 10.1.2.50\nApplication: SharePoint\nFile-Path: /sites/HR/Salary_Data_2024.xlsx\nResult: SUCCESS\nRisk-Score: 85\n===EVENT END===</code></pre><p>Your task is to configure proper parsing and field extraction so this data becomes useful for security monitoring.</p>",
      "environment": "The data is arriving in index=casb with sourcetype=casb:raw. Sample data has been provided showing the log format."
    },
    "steps": [
      {
        "id": 1,
        "title": "Assess the Current State",
        "type": "pivot",
        "question": "First, let's see what the data looks like currently. Run a search to view recent events and assess the parsing issues.",
        "spl": "index=casb sourcetype=casb:raw earliest=-1h\n| head 10\n| table _time, _raw",
        "explanation": "This search shows you the current state of the data. You'll likely see multiple events merged together and timestamps that don't match the log content.",
        "validation": "Look for: Are multiple '===EVENT START===' markers in a single _raw? Does _time match the Timestamp field in the log?"
      },
      {
        "id": 2,
        "title": "Identify Event Boundaries",
        "type": "reasoning",
        "question": "Looking at the sample data, what pattern should we use for LINE_BREAKER to properly separate events?",
        "options": [
          "Break on newlines: ([\\r\\n]+)",
          "Break before ===EVENT START===: ([\\r\\n]+)(?====EVENT START===)",
          "Break after ===EVENT END===: (===EVENT END===[\\r\\n]+)",
          "Break on Timestamp: ([\\r\\n]+)Timestamp:"
        ],
        "expected_answer": "The correct answer is B: Break before ===EVENT START===. This pattern captures the newlines before the event marker and starts a new event at each ===EVENT START===. Option C would also work but captures more text in the break. Options A and D would result in over-breaking, splitting each log line into separate events.",
        "explanation": "The LINE_BREAKER regex should identify a unique pattern that appears at the start of each event. The ===EVENT START=== marker is ideal because it's consistent and unambiguous."
      },
      {
        "id": 3,
        "title": "Configure Event Breaking",
        "type": "pivot",
        "question": "Create the props.conf configuration to properly break events. What settings do you need?",
        "spl": "[casb:raw]\nLINE_BREAKER = ([\\r\\n]+)(?====EVENT START===)\nSHOULD_LINEMERGE = false\nTRUNCATE = 10000",
        "explanation": "This configuration: (1) Breaks events before each ===EVENT START=== marker, (2) Disables line merging since our LINE_BREAKER handles multi-line events, (3) Sets a reasonable truncation limit for safety.",
        "validation": "After applying this configuration (in a test environment or data preview), each event should contain exactly one ===EVENT START=== to ===EVENT END=== block."
      },
      {
        "id": 4,
        "title": "Configure Timestamp Extraction",
        "type": "pivot",
        "question": "The timestamp in the logs uses ISO 8601 format with milliseconds. Configure TIME_PREFIX and TIME_FORMAT to extract it correctly.",
        "spl": "[casb:raw]\nTIME_PREFIX = Timestamp:\\s*\nTIME_FORMAT = %Y-%m-%dT%H:%M:%S.%3NZ\nMAX_TIMESTAMP_LOOKAHEAD = 50",
        "explanation": "TIME_PREFIX identifies the text immediately before the timestamp ('Timestamp: '). TIME_FORMAT uses strptime codes to parse the ISO 8601 format. MAX_TIMESTAMP_LOOKAHEAD ensures we look far enough into the event to find the timestamp.",
        "validation": "After applying, _time should reflect the actual event timestamp, not the indexing time."
      },
      {
        "id": 5,
        "title": "Create Field Extractions",
        "type": "pivot",
        "question": "Create EXTRACT configurations for the key fields: event_type, user, src_ip, application, result, and risk_score.",
        "spl": "[casb:raw]\nEXTRACT-event_type = Event-Type:\\s*(?<event_type>\\w+)\nEXTRACT-user = User-Principal:\\s*(?<user>[^\\r\\n]+)\nEXTRACT-src_ip = Source-IP:\\s*(?<src_ip>\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\nEXTRACT-application = Application:\\s*(?<application>[^\\r\\n]+)\nEXTRACT-result = Result:\\s*(?<result>\\w+)\nEXTRACT-risk_score = Risk-Score:\\s*(?<risk_score>\\d+)",
        "explanation": "Each EXTRACT uses a named capture group to pull the value after the field label. Note that [^\\r\\n]+ captures everything until the end of the line, while \\w+ captures only word characters.",
        "validation": "Search the data and verify all six fields appear in the fields sidebar with correct values."
      },
      {
        "id": 6,
        "title": "Reasoning: CIM Field Mapping",
        "type": "reasoning_checkpoint",
        "question": "The extracted fields use vendor-specific names. Which CIM field mappings are needed for the Authentication data model?",
        "options": [
          "user -> user, src_ip -> src, result -> action, application -> app",
          "user_principal -> user, source_ip -> src_ip, result -> status",
          "user -> account, src_ip -> source, result -> outcome",
          "No mapping needed - the extracted names are already CIM-compliant"
        ],
        "expected_answer": "The correct answer is A. The CIM Authentication data model uses: 'user' (already correct), 'src' for source address, 'action' for success/failure, and 'app' for application. Our extractions already have 'user' correctly named, but we need to alias src_ip -> src, result -> action, and application -> app.",
        "reasoning_checkpoint": true,
        "explanation": "CIM compliance enables your data to work with Enterprise Security and cross-source correlation. The Authentication data model expects specific field names."
      },
      {
        "id": 7,
        "title": "Add CIM Field Aliases",
        "type": "pivot",
        "question": "Add field aliases to map extracted fields to CIM-standard names.",
        "spl": "[casb:raw]\nFIELDALIAS-src = src_ip AS src\nFIELDALIAS-app = application AS app\nEVAL-action = case(result==\"SUCCESS\", \"success\", result==\"FAILURE\", \"failure\", 1==1, \"unknown\")",
        "explanation": "FIELDALIAS creates an additional name for the field without changing the original. EVAL-action normalizes the result values to CIM-standard action values (lowercase).",
        "validation": "Search should now work with both vendor names (src_ip) and CIM names (src)."
      },
      {
        "id": 8,
        "title": "Validate Field Extraction",
        "type": "pivot",
        "question": "Run a validation search to verify all fields are extracting correctly across multiple events.",
        "spl": "index=casb sourcetype=casb:raw earliest=-1h\n| stats count as events,\n        dc(user) as unique_users,\n        dc(src_ip) as unique_sources,\n        values(event_type) as event_types,\n        values(action) as actions",
        "explanation": "This search confirms that fields are extracting with reasonable values. You should see multiple event types, proper action values, and realistic counts.",
        "validation": "All fields should have values. Actions should show 'success', 'failure', or 'unknown'. Event types should match what you saw in the raw data."
      },
      {
        "id": 9,
        "title": "Check CIM Compliance",
        "type": "pivot",
        "question": "Verify the data would appear correctly in the Authentication data model.",
        "spl": "index=casb sourcetype=casb:raw event_type=USER_LOGIN earliest=-1h\n| table _time, user, src, app, action\n| head 20",
        "explanation": "This search uses CIM field names. If aliases are working, you'll see populated columns. This is exactly what an ES analyst would see when investigating authentication events.",
        "validation": "All columns should have values. The 'src' column should show IP addresses (from the src_ip alias). The 'action' column should show normalized values."
      },
      {
        "id": 10,
        "title": "Final Assessment",
        "type": "reasoning",
        "question": "You've completed the onboarding. What should you do before declaring this data source production-ready?",
        "options": [
          "Nothing - the configuration is complete",
          "Wait 24 hours and validate event counts match source system",
          "Create a saved search to monitor data quality",
          "Both B and C - validate completeness and set up monitoring"
        ],
        "expected_answer": "The correct answer is D. Production-ready data requires both validation (confirming expected volumes are arriving) and ongoing monitoring (alerting if data stops flowing or quality degrades). Configuration alone doesn't guarantee the data pipeline is working end-to-end.",
        "explanation": "Data onboarding isn't complete until you've verified completeness against source metrics and established monitoring for ongoing health."
      }
    ],
    "conclusion": {
      "title": "Onboarding Complete",
      "summary": "You've successfully onboarded a custom application log from unusable raw data to CIM-compliant events ready for security analysis. The key steps were: (1) identifying event boundaries, (2) configuring proper timestamp extraction, (3) creating field extractions, (4) normalizing to CIM standards, and (5) validating the complete pipeline.",
      "key_takeaways": [
        "Always analyze raw data first to understand the format",
        "Event breaking must happen before timestamp extraction works correctly",
        "CIM normalization enables cross-source correlation and ES compatibility",
        "Validation and monitoring are essential parts of onboarding",
        "Use data preview to test configurations before production deployment"
      ],
      "next_steps": [
        "Apply these patterns to other data sources in your environment",
        "Create a data quality dashboard for this sourcetype",
        "Document the sourcetype for other analysts",
        "Consider adding event types for data model membership"
      ]
    }
  }
}
